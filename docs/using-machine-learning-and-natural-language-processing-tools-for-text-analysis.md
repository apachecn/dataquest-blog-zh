# ä½¿ç”¨æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·è¿›è¡Œæ–‡æœ¬åˆ†æ

> åŸæ–‡ï¼š<https://www.dataquest.io/blog/using-machine-learning-and-natural-language-processing-tools-for-text-analysis/>

February 28, 2022![](img/ca7bd0308cc1e6822b1d947812edc0a1.png)

è¿™æ˜¯å…³äºå¼•å¯¼é¡¹ç›®åé¦ˆåˆ†æä¸»é¢˜çš„ç¬¬ä¸‰ç¯‡æ–‡ç« ã€‚è¯¥ä¸»é¢˜çš„ä¸»è¦æ€æƒ³æ˜¯åˆ†æå­¦ä¹ è€…åœ¨è®ºå›é¡µé¢ä¸Šæ”¶åˆ°çš„å›å¤ã€‚Dataquest é¼“åŠ±å…¶å­¦ä¹ è€…åœ¨å…¶è®ºå›ä¸Šå‘å¸ƒä»–ä»¬çš„æŒ‡å¯¼é¡¹ç›®ï¼Œåœ¨å‘å¸ƒåï¼Œå…¶ä»–å­¦ä¹ è€…æˆ–å·¥ä½œäººå‘˜å¯ä»¥åˆ†äº«ä»–ä»¬å¯¹è¯¥é¡¹ç›®çš„æ„è§ã€‚æˆ‘ä»¬å¯¹è¿™äº›è§‚ç‚¹çš„å†…å®¹æ„Ÿå…´è¶£ã€‚

åœ¨æˆ‘ä»¬çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](https://www.dataquest.io/blog/how-to-clean-and-prepare-your-data-for-analysis/)ä¸­ï¼Œæˆ‘ä»¬å·²ç»å¯¹æ•°å­—æ•°æ®è¿›è¡Œäº†åŸºæœ¬çš„æ•°æ®åˆ†æï¼Œå¹¶æ·±å…¥åˆ†æäº†åé¦ˆæ–‡ç« çš„æ–‡æœ¬æ•°æ®ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•ä½¿ç”¨å¤šä¸ªåŒ…æ¥å¢å¼ºæˆ‘ä»¬çš„æ–‡æœ¬åˆ†æã€‚æˆ‘ä»¬ä¸ä¼šä¸ºä¸€é¡¹ä»»åŠ¡è®¾å®šç›®æ ‡ï¼Œè€Œæ˜¯ä¼šå°è¯•å„ç§å·¥å…·ï¼Œè¿™äº›å·¥å…·ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œ/æˆ–æœºå™¨å­¦ä¹ æ¥æä¾›è¾“å‡ºã€‚

ä¸‹é¢æ˜¯æˆ‘ä»¬å°†åœ¨æœ¬æ–‡ä¸­å°è¯•çš„ä¸€äº›å¥½ä¸œè¥¿:

*   æƒ…æ„Ÿåˆ†æ
*   å…³é”®è¯æå–
*   ä¸»é¢˜å»ºæ¨¡
*   k å‡å€¼èšç±»
*   ä¸€è‡´
*   æŸ¥è¯¢å›ç­”æ¨¡å‹

ä½¿ç”¨æœºå™¨å­¦ä¹ è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†æ˜¯ä¸€ä¸ªéå¸¸å¹¿æ³›çš„è¯é¢˜ï¼Œä¸å¯èƒ½åŒ…å«åœ¨ä¸€ç¯‡æ–‡ç« ä¸­ã€‚æ‚¨å¯èƒ½ä¼šå‘ç°ï¼Œä»æ‚¨çš„è§’åº¦æ¥çœ‹ï¼Œæœ¬æ–‡ä¸­æè¿°çš„å·¥å…·å¹¶ä¸é‡è¦ã€‚æˆ–è€…å®ƒä»¬è¢«é”™è¯¯åœ°ä½¿ç”¨ï¼Œå®ƒä»¬ä¸­çš„å¤§å¤šæ•°æ²¡æœ‰è¢«è°ƒæ•´ï¼Œæˆ‘ä»¬åªæ˜¯ä½¿ç”¨äº†å¼€ç®±å³ç”¨çš„å‚æ•°ã€‚è¯·è®°ä½ï¼Œè¿™æ˜¯å¯¹ç”¨äºå¢å¼ºåé¦ˆæ•°æ®åˆ†æçš„è½¯ä»¶åŒ…ã€å·¥å…·å’Œæ¨¡å‹çš„ä¸»è§‚é€‰æ‹©ã€‚ä½†æ˜¯ä½ ä¹Ÿè®¸èƒ½æ‰¾åˆ°æ›´å¥½çš„å·¥å…·æˆ–ä¸åŒçš„æ–¹æ³•ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨è¿™æ ·åšå¹¶åˆ†äº«æ‚¨çš„å‘ç°ã€‚

# ML ä¸ NLP

æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ˜¯ä¸¤ä¸ªéå¸¸å®½æ³›çš„æœ¯è¯­ï¼Œå¯ä»¥æ¶µç›–æ–‡æœ¬åˆ†æå’Œå¤„ç†é¢†åŸŸã€‚æˆ‘ä»¬ä¸ä¼šè¯•å›¾åœ¨è¿™ä¸¤ä¸ªæœ¯è¯­ä¹‹é—´è®¾å®šä¸€æ¡å›ºå®šçš„ç•Œé™ï¼Œæˆ‘ä»¬ä¼šæŠŠå®ƒç•™ç»™å“²å­¦å®¶ã€‚å¦‚æœä½ æœ‰å…´è¶£æ¢ç©¶å®ƒä»¬ä¹‹é—´çš„å·®å¼‚ï¼Œè¯·çœ‹è¿™é‡Œã€‚

# æƒ…æ„Ÿåˆ†æ

æƒ…æ„Ÿåˆ†ææ˜¯ ML power çš„ä¸€ä¸ªéå¸¸æµè¡Œçš„åº”ç”¨ã€‚é€šè¿‡åˆ†ææ¯ä¸ªæ–‡æœ¬çš„å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°å¥å­æˆ–æ•´ä¸ªæ–‡æœ¬çš„æƒé‡æ˜¯ç§¯æè¿˜æ˜¯æ¶ˆæã€‚å¦‚æœä½ æƒ³è¿‡æ»¤æ‰å¯¹ä½ äº§å“çš„è´Ÿé¢è¯„ä»·æˆ–è€…åªå±•ç¤ºå¥½çš„è¯„ä»·ï¼Œè¿™å°†ä¼šæœ‰å·¨å¤§çš„ä»·å€¼ã€‚

ç½‘ä¸Šæœ‰å¤§é‡çš„ python æƒ…æ„Ÿåˆ†ææ¨¡å‹å’Œå·¥å…·ã€‚æˆ‘ä»¬å°†å…³æ³¨æœ€ç®€å•çš„ä¸€ä¸ª:æ‰§è¡Œä¸€ä¸ªåŸºæœ¬çš„æƒ…æ„Ÿåˆ†æéœ€è¦ä¸¤è¡Œä»£ç :

```py
# import the package:
from pattern.en import sentiment
# perform the analysis:
x = 'project looks amazing, great job'
sentiment(x)
```

è¾“å‡º:

```py
(0.7000000000000001, 0.825)
```

å¦‚ä¸Šæ‰€ç¤ºï¼Œå¯¼å…¥åŒ…åï¼Œæˆ‘ä»¬åªéœ€è°ƒç”¨æƒ…æ„Ÿå‡½æ•°ï¼Œä¸ºå®ƒæä¾›ä¸€ä¸ªå­—ç¬¦ä¸²å€¼ï¼Œç§ï¼è¾“å‡ºæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå€¼æ˜¯â€œææ€§â€(å¥å­åœ¨ä»-1 åˆ° 1 çš„èŒƒå›´å†…æœ‰å¤šç§¯æ)ã€‚ç¬¬äºŒä¸ªå€¼æ˜¯ä¸»è§‚æ€§ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ç®—æ³•å¯¹å…¶ç¬¬ä¸€ä¸ªå€¼çš„è¯„ä¼°æœ‰å¤šç¡®å®šï¼Œè¿™æ¬¡æ ‡åº¦ä» 0 å¼€å§‹ï¼Œåˆ° 1 ç»“æŸã€‚è®©æˆ‘ä»¬å†çœ‹å‡ ä¸ªä¾‹å­:

```py
y = 'plot looks terrible, spines are too small'
sentiment(y)
```

è¾“å‡º:

```py
(-0.625, 0.7)
```

æˆ‘ä»¬å¾—åˆ°çš„æ˜¯ä¸€ä¸ªç›¸å½“â€œä½â€çš„ç¬¬ä¸€å€¼ï¼Œç®—æ³•å¯¹å…¶ææ€§è¯„ä¼°ä»ç„¶å¾ˆæœ‰ä¿¡å¿ƒã€‚è®©æˆ‘ä»¬è¯•è¯•æ›´éš¾çš„:

```py
z = 'improve the comment, first line looks bad'
sentiment(z)
```

è¾“å‡º:

```py
(-0.22499999999999992, 0.5)
```

æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°æ¨¡å‹åœ¨è¿™ä¸€ç‚¹ä¸Šæ›´åŠ çŠ¹è±«ã€‚å­—ç¬¦ä¸²æœ‰ä¸€ä¸ªå¦å®šè¯ï¼Œä½†å®ƒä¸å†é‚£ä¹ˆç¡®å®šäº†ã€‚

è®©æˆ‘ä»¬å°†æƒ…æ„Ÿå‡½æ•°åº”ç”¨äºæˆ‘ä»¬çš„åé¦ˆå†…å®¹:

```py
df['sentiment'] = df['feedback_clean2_lem'].apply(lambda x: sentiment(x))
df['polarity'] = df['sentiment'].str[0]
df['subjectivity'] = df['sentiment'].str[1]
df = df.drop(columns='sentiment')
```

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»è¡¡é‡äº†æ¯ä¸ªåé¦ˆå¸–å­çš„ææ€§å’Œä¸»è§‚æ€§ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ¯ä¸ªä¸»é¢˜æœ‰ä½•ä¸åŒ:

```py
top15 = df['short_title'].value_counts()[:15].index
df[df['short_title'].isin(top15)].groupby('short_title')[['polarity','subjectivity']].mean().sort_values('subjectivity')
```

| ç®€çŸ­æ ‡é¢˜ | ææ€§ | ä¸»è§‚æ€§ |
| --- | --- | --- |
| sql ä½¿ç”¨ | 0.227535 | 0.441175 |
| ä¸­æƒ…å±€å®å†µæŠ¥é“ | 0.232072 | 0.460941 |
| æ±½è½¦ä»·æ ¼ | 0.206395 | 0.476976 |
| æ˜“è¶£æ±½è½¦ | 0.251605 | 0.498947 |
| äº¤é€šæ‹¥æŒ¤ | 0.219977 | 0.504832 |
| æ˜Ÿçƒå¤§æˆ˜ | 0.250845 | 0.50871 |
| æ–°é—»é»‘å®¢ | 0.288198 | 0.509783 |
| ç¦»èŒå‘˜å·¥ | 0.269066 | 0.512406 |
| ç§‘æ™® | 0.276232 | 0.514718 |
| app ç›ˆåˆ© | 0.281144 | 0.514833 |
| çº½çº¦é«˜ä¸­ | 0.288988 | 0.519288 |
| fandango æ”¶è§†ç‡ | 0.265831 | 0.524607 |
| æ€§åˆ«å·®è· | 0.285667 | 0.534309 |
| å¤§å­¦å¯è§†åŒ– | 0.279269 | 0.547273 |
| å¸‚åœºå¹¿å‘Š | 0.279195 | 0.572073 |

ä¸å¹¸çš„æ˜¯ï¼Œç»“æœéå¸¸ç›¸ä¼¼ã€‚è¿™å¹¶ä¸å¥‡æ€ªï¼Œå¤§å¤šæ•°åé¦ˆå¸–å­éƒ½æœ‰éå¸¸ç›¸ä¼¼çš„ç»“æ„ã€‚å¼€å§‹æ—¶ï¼Œä»–ä»¬é€šå¸¸ä¼šåŒ…å«ä¸€ä¸¤å¥å¯¹é¡¹ç›®è¡¨ç¤ºç¥è´ºçš„è¯ã€‚è¿™ç§æ­£é¢çš„å†…å®¹åé¢é€šå¸¸ä¼šè·Ÿç€ä¸€äº›æ‰¹è¯„æ€§çš„å¤‡æ³¨(é€šå¸¸è¢«è§†ä¸ºå¸¦æœ‰è´Ÿææ€§çš„å†…å®¹)ã€‚

å¸–å­é€šå¸¸ä»¥ä¸€äº›å¯¹æœªæ¥ç¼–ç æœ‰ç›Šçš„ä¿¡æ¯ç»“å°¾ã€‚æœ¬è´¨ä¸Šï¼Œè¿™æ˜¯ä¸€å †ç§¯æå’Œæ¶ˆææƒ…ç»ªäº¤ç»‡åœ¨ä¸€èµ·çš„ä¿¡æ¯ã€‚ä¸åƒäº§å“è¯„è®ºé‚£ä¹ˆç®€å•ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°ä¸€ä¸ªå¼€å¿ƒçš„å®¢æˆ·æˆ–ä¸€ä¸ªéå¸¸ä¸å¼€å¿ƒçš„å®¢æˆ·ã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œå¯¹ä»–ä»¬çš„è¯„è®ºè¿›è¡Œåˆ†ç±»å¹¶ä¸å›°éš¾ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å†…å®¹æ›´åŠ å¤æ‚ã€‚ä½†æ˜¯æˆ‘ä»¬ä¸ä¼šè¿™ä¹ˆè½»æ˜“æ”¾å¼ƒã€‚

# å…³é”®è¯

ä»ç»™å®šçš„å­—ç¬¦ä¸²ä¸­æå–å…³é”®å­—æ˜¯å¦ä¸€ä¸ªé‡è¦çš„æŠ€å·§ï¼Œå¯ä»¥æ”¹è¿›æˆ‘ä»¬çš„åˆ†æã€‚

Rake åŒ…æä¾›äº†ä»æ–‡æœ¬ä¸­æå–çš„æ‰€æœ‰ n å…ƒæ–‡æ³•åŠå…¶æƒé‡çš„åˆ—è¡¨ã€‚è¯¥å€¼è¶Šé«˜ï¼Œæ‰€è€ƒè™‘çš„ n-gram å°±è¶Šé‡è¦ã€‚è§£æå®Œæ–‡æœ¬åï¼Œæˆ‘ä»¬å¯ä»¥åªè¿‡æ»¤æ‰å…·æœ‰æœ€é«˜å€¼çš„ n å…ƒè¯­æ³•ã€‚

è¯·æ³¨æ„ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨åœç”¨è¯æ¥è¯„ä¼°å¥å­ä¸­å“ªäº›è¯æ˜¯é‡è¦çš„ã€‚å¦‚æœæˆ‘ä»¬ç»™è¿™ä¸ªæ¨¡å‹è¾“å…¥æ¸…é™¤äº†åœç”¨è¯çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬ä¸ä¼šå¾—åˆ°ä»»ä½•ç»“æœã€‚

```py
from rake_nltk import Rake
# set the parameteres (length of keyword phrase):
r = Rake(include_repeated_phrases=False, min_length=1, max_length=3)
text_to_rake = df['feedback'][31]
r.extract_keywords_from_text(text_to_rake)
# filter out only the top keywords:
words_ranks = [keyword for keyword in r.get_ranked_phrases_with_scores() if keyword[0] > 5]
words_ranks
```

è¾“å‡º:

```py
[(9.0, 'â€œ professional â€'),
 (9.0, 'avoiding colloquial language'),
 (8.0, 'nicely structured project'),
 (8.0, 'also included antarctica'),
 (8.0, 'add full stops')]
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒRake è®¤ä¸ºâ€œä¸“ä¸šâ€æˆ–â€œé¿å…å£è¯­åŒ–è¯­è¨€â€æ˜¯è¾“å…¥æ–‡æœ¬ä¸­æœ€é‡è¦çš„å…³é”®è¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥åˆ†æï¼Œæˆ‘ä»¬ä¸ä¼šå¯¹å…³é”®å­—çš„æ•°å€¼æ„Ÿå…´è¶£ã€‚æˆ‘ä»¬åªæƒ³æ”¶åˆ°æ¯ä¸ªå¸–å­çš„å‡ ä¸ªçƒ­é—¨å…³é”®è¯ã€‚æˆ‘ä»¬å°†è®¾è®¡ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œåªæå–é¡¶éƒ¨çš„å…³é”®å­—ï¼Œå¹¶å°†å…¶åº”ç”¨äºâ€œåé¦ˆâ€åˆ—:

```py
def rake_it(text):
    r.extract_keywords_from_text(text)
    r.get_ranked_phrases()
    keyword_rank = [keyword for keyword in r.get_ranked_phrases_with_scores() if keyword[0] > 5]
    # select only the keywords and return them:
    keyword_list = [keyword[1] for keyword in keyword_rank]
    return keyword_list

df['rake_words'] = df['feedback'].apply(lambda x: rake_it(x))
```

æŠŠæ¯ä¸ªå¸–å­çš„å…³é”®è¯éƒ½æå–å‡ºæ¥äº†ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å“ªäº›æ˜¯æœ€å—æ¬¢è¿çš„ï¼è®°ä½å®ƒä»¬æ˜¯ä½œä¸ºä¸€ä¸ªåˆ—è¡¨å­˜å‚¨åœ¨ä¸€ä¸ªå•å…ƒæ ¼ä¸­çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å¤„ç†è¿™ä¸ªéšœç¢:

```py
# function from: towardsdatascience.com/dealing-with-list-values-in-pandas-dataframes-a177e534f173
def to_1D(series):
    return pd.Series([x for _list in series for x in _list])

to_1D(df['rake_words']).value_counts()[:10]
```

è¾“å‡º:

```py
jupyter file menu        24
guided project use       24
happy coding :)          22
everything looks nice    20
sql style guide          16
first guided project     16
everything looks good    15
new topic button         15
jupyter notebook file    14
first code cell          13
dtype: int64
```

# ä¸»é¢˜å»ºæ¨¡

ä¸»é¢˜å»ºæ¨¡å¯ä»¥è®©æˆ‘ä»¬å¿«é€Ÿæ´å¯Ÿæ–‡æœ¬çš„å†…å®¹ã€‚ä¸ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ä¸åŒï¼Œä¸»é¢˜å»ºæ¨¡æ˜¯ä¸€ç§æ›´é«˜çº§çš„å·¥å…·ï¼Œå¯ä»¥æ ¹æ®æˆ‘ä»¬çš„éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚

æˆ‘ä»¬ä¸æ‰“ç®—å†’é™©æ·±å…¥è®¾è®¡å’Œå®ç°è¿™ä¸ªæ¨¡å‹ï¼Œå®ƒæœ¬èº«å¯ä»¥å¡«å……å‡ ç¯‡æ–‡ç« ã€‚æˆ‘ä»¬å°†é’ˆå¯¹æ¯ä¸ªåé¦ˆå†…å®¹å¿«é€Ÿè¿è¡Œè¯¥æ¨¡å‹çš„åŸºæœ¬ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºæ¯ä¸ªå¸–å­æå–æŒ‡å®šæ•°é‡çš„ä¸»é¢˜ã€‚

æˆ‘ä»¬å°†ä»ä¸€ä¸ªåé¦ˆå¸–å­å¼€å§‹ã€‚è®©æˆ‘ä»¬å¯¼å…¥å¿…è¦çš„åŒ…ï¼Œç¼–è¯‘æ–‡æœ¬å¹¶åˆ›å»ºæ‰€éœ€çš„å­—å…¸å’ŒçŸ©é˜µ(è¿™æ˜¯æœºå™¨å­¦ä¹ ï¼Œå› æ­¤æ¯ä¸ªæ¨¡å‹éƒ½éœ€è¦ç‰¹å®šçš„è¾“å…¥):

```py
# Importing:
import gensim
from gensim import corpora
import re

# compile documents, let's try with 1 post:
doc_complete = df['feedback_clean2_lem'][0]
docs = word_tokenize(doc_complete)
docs_out = []
docs_out.append(docs)

# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)
dictionary = corpora.Dictionary(docs_out)

# Converting a list of documents (corpus) into Document Term Matrix using dictionary prepared above.
doc_term_matrix = [dictionary.doc2bow(doc) for doc in docs_out]
```

åšäº†æ‰€æœ‰çš„å‡†å¤‡å·¥ä½œä¹‹åï¼Œå°±è¯¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹å¹¶æå–ç»“æœäº†ã€‚æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨è®¾ç½®è®¸å¤šå‚æ•°ã€‚ä¸¾å‡ ä¸ªä¾‹å­:è¯é¢˜çš„æ•°é‡ï¼Œæ¯ä¸ªè¯é¢˜æˆ‘ä»¬ç”¨äº†å¤šå°‘å•è¯ã€‚ä½†æ˜¯æ¸…å•è¿˜åœ¨ç»§ç»­ï¼Œå°±åƒè®¸å¤š ML æ¨¡å‹ä¸€æ ·ï¼Œä½ å¯ä»¥èŠ±å¾ˆå¤šæ—¶é—´è°ƒæ•´è¿™äº›å‚æ•°æ¥å®Œå–„ä½ çš„æ¨¡å‹ã€‚

```py
# Running and Trainign LDA model on the document term matrix.
Lda = gensim.models.ldamodel.LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50, random_state=4)

# Getting results:
ldamodel.print_topics(num_topics=5, num_words=4)
```

è¾“å‡º:

```py
[(0, '0.032*"notebook" + 0.032*"look" + 0.032*"process" + 0.032*"month"'),
 (1, '0.032*"notebook" + 0.032*"look" + 0.032*"process" + 0.032*"month"'),
 (2, '0.032*"important" + 0.032*"stay" + 0.032*"datasets" + 0.032*"process"'),
 (3,
  '0.032*"httpswww1nycgovsitetlcabouttlctriprecorddatapage" + 0.032*"process" + 0.032*"larger" + 0.032*"clean"'),
 (4, '0.113*"function" + 0.069*"inside" + 0.048*"memory" + 0.048*"ram"')]
```

æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«å•è¯åŠå…¶æƒé‡ã€‚æ•°å­—è¶Šé«˜ï¼Œå•è¯è¶Šé‡è¦(æ ¹æ®æˆ‘ä»¬çš„æ¨¡å‹)ã€‚å¦‚æœæˆ‘ä»¬æƒ³æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬å¯ä»¥æå–æŸä¸ªå€¼ä»¥ä¸Šçš„æ‰€æœ‰å•è¯â€¦åªæ˜¯ä¸€ä¸ªæƒ³æ³•ğŸ™‚

è®©æˆ‘ä»¬ç»§ç»­å°†è¯¥æ¨¡å‹åº”ç”¨äºæ¯ä¸ªåé¦ˆå¸–å­ã€‚ä¸ºäº†ç®€åŒ–æˆ‘ä»¬çš„ç”Ÿæ´»ï¼Œæˆ‘ä»¬å°†åªæå–ä¸»é¢˜è¯ï¼Œè€Œä¸æ˜¯â€œæƒé‡â€å€¼ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å¯¹æå–çš„ä¸»é¢˜æ‰§è¡Œ value_countsï¼Œå¹¶æŸ¥çœ‹å“ªäº›ä¸»é¢˜æ˜¯æœ€å—æ¬¢è¿çš„(æ ¹æ®æ¨¡å‹)ã€‚ä¸ºäº†å¯¹åˆ—ä¸­çš„æ¯ä¸ªå•å…ƒæ ¼è¿›è¡Œä¸»é¢˜å»ºæ¨¡ï¼Œæˆ‘ä»¬å°†è®¾è®¡ä¸€ä¸ªå‡½æ•°ã€‚ä½œä¸ºè¾“å…¥å€¼ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å•å…ƒæ ¼çš„å†…å®¹(æ–‡æœ¬)å’Œä¸»é¢˜çš„å­—æ•°:

```py
def get_topic(x,n):
    """
    extract list of topics given text(x) and number(n) of words for topic
    """
    docs = word_tokenize(x)
    docs_out = []
    docs_out.append(docs)
    dictionary = corpora.Dictionary(docs_out)
    doc_term_matrix = [dictionary.doc2bow(doc) for doc in docs_out]
    Lda = gensim.models.ldamodel.LdaModel
    # Running and Trainign LDA model on the document term matrix.
    ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50, random_state=1)
    topics = ldamodel.print_topics(num_topics=2, num_words=n)
    topics_list = []
    for elm in topics:
        content = elm[1]
        no_digits = ''.join([i for i in content if not i.isdigit()])
        topics_list.append(re.findall(r'\w+', no_digits, flags=re.IGNORECASE))
    return topics_list 
```

è®©æˆ‘ä»¬çœ‹çœ‹æœ€å¤§é•¿åº¦ä¸º 4 ä¸ªå•è¯çš„ä¸»é¢˜æ˜¯ä»€ä¹ˆæ ·å­çš„:

```py
df['topic_4'] = df['feedback_clean2_lem'].apply(lambda x: get_topic(x,4))
to_1D(df['topic_4']).value_counts()[:10]
```

è¾“å‡º:

```py
[keep, nice]                                               8
[nice, perform]                                            4
[nice]                                                     4
[nan]                                                      4
[sale, take, look, finish]                                 2
[learning, keep, saw, best]                                2
[library, timezones, learning, httpspypiorgprojectpytz]    2
[job, graph, please, aesthetically]                        2
[thanks, think, nice, learning]                            2
[especially, job, like, nice]                              2
dtype: int64
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥æœ€å¤š 3 ä¸ªè¯çš„ä¸»é¢˜:

```py
df['topic_3'] = df['feedback_clean2_lem'].apply(lambda x: get_topic(x,3))
to_1D(df['topic_3']).value_counts()[:10]
```

è¾“å‡º:

```py
[keep, nice]                       8
[share, thank, please]             4
[nan]                              4
[nice]                             4
[nice, perform]                    4
[guide, documentation, project]    3
[]                                 3
[cell]                             3
[guide, project, documentation]    3
[plot]                             3
dtype: int64
```

æˆ‘ä»¬çš„ç»“æœä¸æ˜¯å¾ˆå¥½ï¼Œä½†æ˜¯è®°ä½æˆ‘ä»¬åªæ˜¯è§¦åŠäº†è¿™ä¸ªå·¥å…·çš„è¡¨é¢ã€‚è°ˆåˆ° lda æ¨¡å‹ï¼Œæœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚æˆ‘é¼“åŠ±æ‚¨è‡³å°‘é˜…è¯»ä¸‹é¢çš„ä¸€ç¯‡æ–‡ç« ï¼Œä»¥æ‰©å¤§æ‚¨å¯¹è¿™ä¸ªåº“çš„äº†è§£:

*   [æœºæ¢°ç“¶é¢ˆ](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)
*   [èµ°å‘æ•°æ®ç§‘å­¦](https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925)

# k å‡å€¼èšç±»

Kmeans æ¨¡å‹å¯ä»¥åŸºäºå„ç§è¾“å…¥å¯¹æ•°æ®è¿›è¡Œèšç±»ï¼Œè¿™å¯èƒ½æ˜¯æœ€æµè¡Œçš„æ— ç›‘ç£æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚åªéœ€é€‰æ‹©æ‚¨å¸Œæœ›æ•°æ®åˆ†é…åˆ°å¤šå°‘ä¸ªé›†ç¾¤ï¼ŒåŸºäºä»€ä¹ˆåŠŸèƒ½å’Œç§ã€‚ä½œä¸ºä¸€ä¸ª ML æ¨¡å‹ï¼Œæˆ‘ä»¬ä¸èƒ½åªç»™å®ƒæä¾›åŸå§‹æ–‡æœ¬ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹æ–‡æœ¬æ•°æ®è¿›è¡ŒçŸ¢é‡åŒ–ï¼Œç„¶åç»™æ¨¡å‹æä¾›åŸå§‹æ–‡æœ¬ã€‚æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬å°†æ–‡æœ¬æ•°æ®è½¬æ¢æˆæ•°å­—æ•°æ®ã€‚æˆ‘ä»¬å¦‚ä½•åšå–å†³äºæˆ‘ä»¬è‡ªå·±ï¼Œæœ‰è®¸å¤šæ–¹æ³•å¯ä»¥å¯¹æ•°æ®è¿›è¡ŒçŸ¢é‡åŒ–ï¼Œè®©æˆ‘ä»¬è¯•è¯• TfidfVectorizer:

```py
# imports:
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# vectorize text:
tfidfconverter = TfidfVectorizer(max_features=5000, min_df=0.1, max_df=0.7, stop_words=stopwords.words('english'))  
X = tfidfconverter.fit_transform(df['feedback']).toarray()

# fit and label:
Kmean = KMeans(n_clusters=8, random_state=2)
Kmean.fit(X)
df['label'] = Kmean.labels_
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸åŒçš„é›†ç¾¤æ˜¯å¦æœ‰å¾ˆå¤§çš„ææ€§å·®å¼‚:

```py
df.groupby('label')['polarity'].mean()
```

è¾“å‡º:

```py
label
0    0.405397
1    0.312328
2    0.224152
3    0.210876
4    0.143431
5    0.340016
6    0.242555
7    0.241244
Name: polarity, dtype: float64
```

å®é™…ä¸Šï¼Œè®©æˆ‘ä»¬æ ¹æ®æˆ‘ä»¬çš„æ¨¡å‹åˆ†é…çš„é›†ç¾¤ç¼–å·æ£€æŸ¥ä¸€äº›å…¶ä»–çš„ç¼–å·:

```py
 #group data:
polar = df.groupby('label')['polarity'].mean()
subj = df.groupby('label')['subjectivity'].mean()
level = df.groupby('label')['level'].mean()
val_cnt = df['label'].value_counts()
length = df.groupby('label')['len'].mean()

#create a df and rename some of the columns, index:
cluster_df = pd.concat([val_cnt,polar,subj,level, length], axis=1)
cluster_df = cluster_df.rename(columns={'label':'count'})
cluster_df.index.name = 'label'
cluster_df
```

| æ ‡ç­¾ | æ•°æ•° | ææ€§ | ä¸»è§‚æ€§ | æ°´å¹³ | ä½è¾“å…¥è”ç½‘ï¼ˆlow-entry networking çš„ç¼©å†™ï¼‰ |
| --- | --- | --- | --- | --- | --- |
| Zero | Eighty-seven | 0.405397 | 0.635069 | 1.49425 | Three hundred and fourteen point two eight seven |
| one | One hundred and fifty | 0.312328 | 0.536363 | 1.55333 | Seven hundred and forty-two point two five three |
| Two | Sixty | 0.224152 | 0.469265 | One point five | Five hundred and ninety-four point two six seven |
| three | One hundred and thirty-six | 0.210876 | 0.513048 | 1.46324 | One thousand four hundred and twenty-nine point one |
| four | Sixty-six | 0.143431 | 0.34258 | 1.4697 | Two hundred and fifty-one point two two seven |
| five | One hundred and eighteen | 0.340016 | 0.581554 | 1.29661 | Nine hundred and three point one one |
| six | Three hundred and two | 0.242555 | 0.495008 | 1.45033 | Seven hundred and twenty-four point two zero nine |
| seven | Ninety-two | 0.241244 | 0.431905 | 1.52174 | Three hundred and ninety-eight point two two eight |

æˆ‘ä»¬å¯ä»¥åœ¨è¯¥è¡¨ä¸­æ³¨æ„åˆ°ä¸€äº›æœ‰è¶£çš„è¶‹åŠ¿ï¼Œä¾‹å¦‚ï¼Œ0 å·èšç±»å…·æœ‰ç›¸å½“ç§¯æçš„å†…å®¹(é«˜ææ€§å¹³å‡å€¼)ï¼Œæ­¤å¤–ï¼Œæˆ‘ä»¬ä¹‹å‰åœ¨è¯¥èšç±»ä¸­ä½¿ç”¨çš„æƒ…æ„Ÿæ¨¡å‹å¯¹å…¶å»ºæ¨¡ç›¸å½“ç¡®å®š(é«˜ä¸»è§‚æ€§å€¼)ã€‚è¿™å¯èƒ½æ˜¯ç”±è¯¥ç°‡(314)ä¸­éå¸¸çŸ­çš„å¹³å‡æ–‡æœ¬é•¿åº¦å¼•èµ·çš„ã€‚çœ‹ç€ä¸Šé¢çš„è¡¨æ ¼ï¼Œä½ æœ‰æ²¡æœ‰å‘ç°å…¶ä»–æœ‰è¶£çš„äº‹å®ï¼Ÿ

è¯·è®°ä½ï¼Œæˆ‘ä»¬å·²ç»å‘ Kmeans æ¨¡å‹æä¾›äº†ä½¿ç”¨ Tfidf çŸ¢é‡åŒ–çš„æ•°æ®ï¼Œåœ¨å°†æ–‡æœ¬æ•°æ®æä¾›ç»™æ¨¡å‹ä¹‹å‰ï¼Œæœ‰å¤šç§æ–¹å¼[å¯¹æ–‡æœ¬æ•°æ®è¿›è¡ŒçŸ¢é‡åŒ–ã€‚ä½ åº”è¯¥è¯•è¯•å®ƒä»¬ï¼Œçœ‹çœ‹å®ƒä»¬å¯¹ç»“æœæœ‰ä»€ä¹ˆå½±å“ã€‚](https://neptune.ai/blog/vectorization-techniques-in-nlp-guide)

# å¥å­èšç±»

å¦‚å‰æ‰€è¿°:æ¯ä¸ªåé¦ˆå¸–å­çš„å†…å®¹æ˜¯èµç¾å’Œå»ºè®¾æ€§æ‰¹è¯„çš„ç›¸å½“å¤æ‚çš„æ··åˆã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„èšç±»æ¨¡å‹åœ¨è¢«è¦æ±‚å¯¹å¸–å­è¿›è¡Œèšç±»æ—¶è¡¨ç°ä¸ä½³ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å°†æ‰€æœ‰çš„å¸–å­åˆ†æˆå¥å­ï¼Œå¹¶è¦æ±‚æ¨¡å‹å°†å¥å­èšé›†èµ·æ¥ï¼Œæˆ‘ä»¬åº”è¯¥ä¼šæ”¹å–„æˆ‘ä»¬çš„ç»“æœã€‚

```py
from nltk.corpus import stopwords
stop = set(stopwords.words('english'))
from nltk import sent_tokenize
from nltk import pos_tag
import string
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# set up for cleaning and sentence tokenizing:
exclude = set(string.punctuation)
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    return punc_free

# remember this function from article no 2?
def lemmatize_it(sent):
    empty = []
    for word, tag in pos_tag(word_tokenize(sent)):
        wntag = tag[0].lower()
        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
        if not wntag:
            lemma = word
            empty.append(lemma)
        else:
            lemma = lemmatizer.lemmatize(word, wntag)
            empty.append(lemma)
    return ' '.join(empty)

# tokenize sentences and clean them:
doc_complete = sent_tokenize(df['feedback'].sum())
doc_clean = [clean(doc) for doc in doc_complete] 
doc_clean_lemmed = [lemmatize_it(doc) for doc in doc_clean] 

# create and fill a dataframe with sentences:
sentences = pd.DataFrame(doc_clean_lemmed)
sentences.columns = ['sent']
sentences['orig'] = doc_complete
sentences['keywords'] = sentences['orig'].apply(lambda x: rake_it(x))

# vectorize text:
tfidfconverter = TfidfVectorizer(max_features=10, min_df=0.1, max_df=0.9, stop_words=stopwords.words('english'))  
X = tfidfconverter.fit_transform(sentences['sent']).toarray()

# fit and label:
Kmean = KMeans(n_clusters=8)
Kmean.fit(X)
sentences['label'] = Kmean.labels_
```

ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹æ¯ä¸ªæ ‡ç­¾æœ€å—æ¬¢è¿çš„å…³é”®è¯:

```py
to_1D(sentences[sentences['label'] == 0]['keywords']).value_counts()[:10]
```

è¾“å‡º:

```py
everything looks nice     15
everything looks good     13
sql style guide           10
print () function          8
social love attention      7
data cleaning process      7
looks pretty good          6
screen shot 2020           5
everything looks great     5
jupyter notebook file      5
dtype: int64
```

```py
to_1D(sentences[sentences['label'] == 2]['keywords']).value_counts()[:10]
```

è¾“å‡º:

```py
first code cell            8
avoid code repetition      7
items (): spine            4
might appear complex       4
may appear complex         4
might consider creating    3
1st code cell              3
print () function          3
little suggestion would    3
one code cell              3
dtype: int64
```

# èšç±» n å…ƒæ–‡æ³•

ç±»ä¼¼äºå¯¹å¸–å­å’Œå¥å­è¿›è¡Œèšç±»ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ n å…ƒè¯­æ³•è¿›è¡Œèšç±»:

```py
from nltk.util import ngrams 
import collections

# extract n-grams and put them in a dataframe:
tokenized = word_tokenize(sentences['sent'].sum())
trigrams = ngrams(tokenized, 3)
trigrams_freq = collections.Counter(trigrams)
trigram_df = pd.DataFrame(trigrams_freq.most_common())
trigram_df.columns = ['trigram','count']
trigram_df['tgram'] = trigram_df['trigram'].str[0]+' '+trigram_df['trigram'].str[1]+' '+trigram_df['trigram'].str[2]

# vectorize text:
tfidfconverter = TfidfVectorizer(max_features=100, min_df=0.01, max_df=0.9, stop_words=stopwords.words('english'))  
X = tfidfconverter.fit_transform(trigram_df['tgram']).toarray()

# fit and label:
Kmean = KMeans(n_clusters=8, random_state=1)
Kmean.fit(X)
trigram_df['label'] = Kmean.labels_
```

# ä¸€è‡´

[https://avidml . WordPress . com/2017/08/05/natural-language-processing-concordance/](https://avidml.wordpress.com/2017/08/05/natural-language-processing-concordance/)

å¦‚æœæˆ‘ä»¬æƒ³æ£€æŸ¥ä¸€ä¸ªç‰¹å®šçš„å•è¯åœ¨æ–‡æœ¬ä¸­æ˜¯å¦‚ä½•ä½¿ç”¨çš„å‘¢ï¼Ÿæˆ‘ä»¬æƒ³çœ‹çœ‹è¿™ä¸ªç‰¹å®šå•è¯å‰åçš„å•è¯ã€‚åœ¨ concordance çš„ä¸€ç‚¹å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¿«åœ°çœ‹ä¸€çœ‹:

```py
from nltk.text import Text 
text = nltk.Text(word_tokenize(df['feedback'].sum()))
text.concordance("plot", lines=10)
```

è¾“å‡º:

```py
Displaying 10 of 150 matches:
ive precision . it is better to make plot titles bigger . about the interactiv
 '' ] what is the point of your last plot ? does it confirm your hypothesis th
ou use very similar code to create a plot â€“ there is an opportunity to reduce 
er plots â€ try to comment after each plot and not at the end so the reader doe
ur case saleprice , and after it you plot correlation only between remaining f
tation then possible and correlation plot may be have different colors and val
tting the format of the grid on your plot setting the ylabel as â€˜ average traf
 line_data = series that you want to plot # start_hour = beginning hour of the
#start_hour = beginning hour of the plot **in 24hr format** # end_hour = end 
ormat** # end_hour = end hour of the plot **in 24hr format** def plot_traffic_
```

# æŸ¥è¯¢å›ç­”æ¨¡å‹

è€å®è¯´ï¼Œä»ä¸€ä¸ªç®€å•çš„æŸ¥è¯¢å›ç­”æ¨¡å‹å¼€å§‹ï¼Œä½ ä¸éœ€è¦äº†è§£å¤ªå¤šå…³äºæ¨¡å‹å†…éƒ¨å‘ç”Ÿçš„å·«æœ¯çš„å…·ä½“æœºåˆ¶ã€‚æœ‰å¿…è¦äº†è§£ä¸€äº›åŸºæœ¬çŸ¥è¯†:

*   ä½ å¿…é¡»æŠŠæ–‡æœ¬è½¬æ¢æˆå‘é‡å’Œæ•°ç»„
*   è¯¥æ¨¡å‹æ¯”è¾ƒè¯¥æ•°å­—è¾“å…¥ï¼Œå¹¶æ‰¾åˆ°ä¸è¯¥è¾“å…¥æœ€ç›¸ä¼¼çš„å†…å®¹
*   å°±è¿™æ ·ï¼Œåœ¨æ‚¨æˆåŠŸè¿è¡Œäº†ä¸€ä¸ªåŸºæœ¬æ¨¡å‹ä¹‹åï¼Œæ‚¨åº”è¯¥å¼€å§‹å°è¯•ä¸åŒçš„å‚æ•°å’ŒçŸ¢é‡åŒ–æ–¹æ³•

```py
# imports:
from sklearn.metrics import pairwise_distances
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords

stopword_list = stopwords.words('english')

# create a model:
dtm = CountVectorizer(max_df=0.7, min_df=5, token_pattern="[a-z']+", 
                      stop_words=stopword_list, max_features=6000)

dtm.fit(df['feedback_clean2_lem'])
dtm_mat = dtm.transform(df['feedback_clean2_lem'])
tsvd = TruncatedSVD(n_components=200)
tsvd.fit(dtm_mat)
tsvd_mat = tsvd.transform(dtm_mat)

# let's look for "slow function solution"
query = "slow function solution"
query_mat = tsvd.transform(dtm.transform([query]))

# calculate distances:
dist = pairwise_distances(X=tsvd_mat, Y=query_mat, metric='cosine')
# return the post with the smallest distance:
df['feedback'][np.argmin(dist.flatten())]
```

è¾“å‡º:

```py
' processing data inside a function saves memory (the variables you create stay inside the function and are not stored in memory, when you are done with the function) it is important when you are working with larger datasets - if you are interested with experimenting: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page try cleaning 1 month of this dataset on kaggle notebook (and look at your ram usage) outside the function and inside the function, compare the ram usage in both examples '
```

è¯·è®°ä½ï¼Œæˆ‘ä»¬æ­£åœ¨è§£æä»¥å¯»æ‰¾ç­”æ¡ˆçš„æ•°æ®é›†ç›¸å½“å°ï¼Œå› æ­¤æˆ‘ä»¬ä¸èƒ½æœŸå¾…ä»¤äººå…´å¥‹çš„ç­”æ¡ˆã€‚

# æœ€åä¸€å¥è¯

è¿™å¹¶ä¸æ˜¯ç”¨äºæ–‡æœ¬åˆ†æçš„ä¸€é•¿ä¸²å·¥å…·çš„ç»“å°¾ã€‚æˆ‘ä»¬å‡ ä¹æ²¡æœ‰è§¦åŠè¡¨é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„å·¥å…·è¿˜æ²¡æœ‰å¾—åˆ°æœ€æœ‰æ•ˆçš„åˆ©ç”¨ã€‚ä½ åº”è¯¥ç»§ç»­å¯»æ‰¾æ›´å¥½çš„æ–¹æ³•ï¼Œè°ƒæ•´æ¨¡å‹ï¼Œä½¿ç”¨ä¸åŒçš„çŸ¢é‡å™¨ï¼Œæ”¶é›†æ›´å¤šçš„æ•°æ®ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†å’Œ[æœºå™¨å­¦ä¹ ](https://www.dataquest.io/course/machine-learning-fundamentals/)çš„ç–¯ç‹‚æ··åˆï¼Œæ˜¯ä¸€ä¸ªå¯ä»¥ç ”ç©¶å‡ åå¹´çš„æ°¸æ— æ­¢å¢ƒçš„è¯é¢˜ã€‚å°±åœ¨è¿‡å»çš„ 20 å¹´é‡Œï¼Œè¿™äº›å·¥å…·ç»™æˆ‘ä»¬å¸¦æ¥äº†æƒŠäººçš„åº”ç”¨ï¼Œä½ è¿˜è®°å¾—è°·æ­Œä¹‹å‰çš„ä¸–ç•Œå—ï¼Ÿåœ¨ç½‘ä¸Šæœç´¢å†…å®¹å’Œçœ‹é»„é¡µéå¸¸ç›¸ä¼¼ã€‚ç”¨æˆ‘ä»¬çš„æ™ºèƒ½æ‰‹æœºåŠ©æ‰‹æ€ä¹ˆæ ·ï¼Ÿè¿™äº›å·¥å…·ä¸æ–­å˜å¾—æ›´æœ‰æ•ˆï¼Œå€¼å¾—ä½ å…³æ³¨å®ƒä»¬å¦‚ä½•å˜å¾—æ›´å¥½åœ°ç†è§£æˆ‘ä»¬çš„è¯­è¨€ã€‚

## æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ

éšä¾¿ä¼¸æ‰‹é—®æˆ‘ä»€ä¹ˆ:
[Dataquest](https://community.dataquest.io/u/adam.kubalica/summary) ï¼Œ [LinkedIn](https://www.linkedin.com/in/kubalica/) ï¼Œ [GitHub](https://github.com/grumpyclimber/portfolio)
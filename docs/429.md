# PyTorch 初学者入门(2022)

> 原文:[https://www.dataquest.io/blog/pytorch-for-beginners/](https://www.dataquest.io/blog/pytorch-for-beginners/)

November 14, 2022![PyTorch for Beginners](../Images/f6308c003fddf674099e2eaae75f8e35.png)

当使用 Python 进行机器学习时，对于使用哪个库或框架，您有多种选择。然而，如果你正在走向深度学习，你可能应该使用 TensorFlow 或 PyTorch，这两个最著名的深度学习框架。

在本文中，我们将快速介绍 PyTorch 框架，从最初的概念一直到第一个图像分类模型的训练和测试。

我们不会深入学习复杂的概念和数学，因为本文旨在成为一种更具实践性的方法，介绍如何将 PyTorch 作为一种工具开始，而不是将深度学习作为一种概念。

因此，我们假设你有一些中级 Python 知识——包括类和面向对象编程——并且你熟悉深度学习的主要概念。

## PyTorch

PyTorch 是一个强大而易用的 Python 深度学习库，主要用于计算机视觉和自然语言处理等应用。

TensorFlow 是由谷歌开发的，PyTorch 是由脸书的人工智能研究小组开发的，该小组最近将该框架的管理工作移交给了新成立的 PyTorch 基金会，该基金会受 Linux 基金会的监管。

PyTorch 的灵活性允许轻松集成新的数据类型和算法，并且该框架也是高效的和可伸缩的，因为它被设计为最小化所需的计算量并与各种硬件架构兼容。

## 张量

在深度学习中，张量是一种非常类似于数组和矩阵的基本数据结构，利用它我们可以高效地对大数据集进行数学运算。张量可以表示为矩阵，也可以表示为向量、标量或更高维的数组。

为了更容易形象化，可以把张量想象成一个包含标量或其他数组的简单数组。在 PyTorch 上，张量是一种非常类似于`ndarray`的结构，不同之处在于它们能够在 GPU 上运行，这大大加快了计算过程。

从一个数字创建一个张量很简单:

```
import torch
import numpy as np

ndarray = np.array([0, 1, 2])
t = torch.from_numpy(ndarray)
print(t)
```

```
 tensor([0, 1, 2])
```

PyTorch 上的张量有三个属性:

*   **形状**:张量的大小
*   **数据类型**:存储在张量中的数据类型
*   **设备**:存储张量的设备

如果我们打印我们创建的张量的属性，我们将得到如下结果:

```
print(t.shape)
print(t.dtype)
print(t.device)
```

```
 torch.Size([3])
    torch.int64
    cpu
```

这意味着我们有一个大小为 3 的一维张量，包含存储在 CPU 中的整数。

我们也可以从 Python 列表中实例化张量:

```
t = torch.tensor([0, 1, 2])
print(t)
```

```
 tensor([0, 1, 2])
```

张量也可以是多维的:

```
ndarray = np.array([[0, 1, 2], [3, 4, 5]])
t = torch.from_numpy(ndarray)
print(t)
```

```
 tensor([[0, 1, 2],
            [3, 4, 5]])
```

也有可能从另一个张量创建一个张量。在这种情况下，新张量继承了初始张量的特征。以下示例基于之前创建的张量创建了一个包含随机数的张量:

```
new_t = torch.rand_like(t, dtype=torch.float)
print(new_t)
```

```
 tensor([[0.1366, 0.5994, 0.3963],
            [0.1126, 0.8860, 0.8233]])
```

请注意，`rand_like()`函数创建了一个形状为(2，2)的新张量。然而，由于函数返回值从 0 到 1，我们不得不覆盖数据类型为 float。

我们也可以简单地从我们期望的形状创建一个随机张量:

```
my_shape = (3, 3)
rand_t = torch.rand(my_shape)
print(rand_t)
```

```
 tensor([[0.8099, 0.8816, 0.3071],
            [0.1003, 0.3190, 0.3503],
            [0.9088, 0.0844, 0.0547]])
```

### 张量运算

就像在 NumPy 中一样，我们可以对张量执行多种可能的操作，比如切片、转置、矩阵相乘等等。

张量的切片与 Python 中的任何其他数组结构完全一样。考虑下面的张量:

```
zeros_tensor = torch.zeros((2, 3))
print(zeros_tensor)
```

```
 tensor([[0., 0., 0.],
            [0., 0., 0.]])
```

我们可以很容易地索引第一行或第一列:

```
print(zeros_tensor[1])
print(zeros_tensor[:, 0])
```

```
 tensor([0., 0., 0.])
    tensor([0., 0.])
```

我们也可以把这个张量转置:

```
transposed = zeros_tensor.T
print(transposed)
```

```
 tensor([[0., 0.],
            [0., 0.],
            [0., 0.]])
```

最后，我们可以将张量相乘:

```
ones_tensor = torch.ones(3, 3)
product = torch.matmul(zeros_tensor, ones_tensor)
print(product)
```

```
 tensor([[0., 0., 0.],
            [0., 0., 0.]])
```

请注意，我们使用了`zeros`和`ones`函数来创建一个只包含 0 和 1 的张量，它带有我们传递的形状。

这些操作只是 PyTorch 所能做的一小部分。然而，本文的目的并不是要涵盖它们中的每一个，而是给出它们如何工作的一般概念。如果你想了解更多，PyTorch 有完整的[文档](https://pytorch.org/tutorials/)。

## 加载数据

PyTorch 自带一个内置模块，为许多深度学习应用程序提供现成的数据集，如计算机视觉、语音识别和自然语言处理。这意味着有可能建立自己的神经网络，而不需要自己收集和处理数据。

例如，我们将下载 MNIST 数据集。MNIST 是手写数字图像的数据集，包含 6 万个样本和 1 万个图像的测试集。

我们将使用来自`torchvision`的`datasets`模块下载数据:

```
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt

training_data = datasets.MNIST(root=".", train=True, download=True, transform=ToTensor())

test_data = datasets.MNIST(root=".", train=False, download=True, transform=ToTensor())
```

```
 Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz

      0%|          | 0/9912422 [00:00<?, ?it/s]

    Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz

      0%|          | 0/28881 [00:00<?, ?it/s]

    Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz

      0%|          | 0/1648877 [00:00<?, ?it/s]

    Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz

      0%|          | 0/4542 [00:00<?, ?it/s]

    Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw
```

在下载函数中，我们有以下参数:

1.  **root** :保存数据的目录。您可以传递带有目录路径的字符串。一个点(如示例中所示)将文件保存在您所在的目录中。

2.  **train** :用于通知 PyTorch 你正在下载 train 还是测试集。

3.  **下载**:如果您指定的路径已经没有数据，是否下载。

4.  **转换**:对数据进行转换。在我们的代码中，我们选择张量。

如果我们打印训练集的第一个元素，我们将看到以下内容:

```
training_data[0]
```

```
 (tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,
               0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,
               0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,
               0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,
               0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,
               0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,
               0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,
               0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,
               0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,
               0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,
               0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,
               0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,
               0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,
               0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,
               0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,
               0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,
               0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,
               0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,
               0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000],
              [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
               0.0000, 0.0000, 0.0000, 0.0000]]]), 5)
```

```
[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,
0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000],
[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,
0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000],
[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000],
[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,
0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000],
[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,
0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000],
[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,
0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000],
[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,
0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
0.0000, 0.0000, 0.0000, 0.0000],
```

上面的张量只是整个元素的一小部分，因为它太大而无法显示。

这一串数字对我们来说可能没有任何意义，因为它们代表图像，所以我们可以使用 matplotlib 将它们可视化为实际图像:

```
figure = plt.figure(figsize=(8, 8)) 
cols, rows = 5, 5

for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(training_data), size=(1,)).item()
    img, label = training_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray")
plt.show()
```

![output_25_0.png](../Images/cc6e9ff10e4ab4d2aefd83c55e8dec17.png)

我们还可以使用`classes`属性来查看数据中的类:

```
training_data.classes
```

```
 ['0 - zero',
     '1 - one',
     '2 - two',
     '3 - three',
     '4 - four',
     '5 - five',
     '6 - six',
     '7 - seven',
     '8 - eight',
     '9 - nine']
```

当模型被训练时，它可以接收新的输入，然后分类为这些类别之一。

现在我们已经下载了数据，我们将使用`DataLoader`。这使我们能够以小批量迭代数据集，而不是一次一个观察，并在训练模型的同时洗牌数据。代码如下:

```
from torch.utils.data import DataLoader

loaded_train = DataLoader(training_data, batch_size=64, shuffle=True)
loaded_test = DataLoader(test_data, batch_size=64, shuffle=True)
```

## 神经网络

在深度学习中，神经网络是一种用于以复杂模式对数据进行建模的算法。神经网络试图通过由处理节点连接的多个层来模拟人脑的功能，其行为类似于人类神经元。通过节点连接的这些层创建了一个能够处理和理解大量复杂数据的复杂网络。

在 PyTorch 中，与神经网络相关的一切都是使用`torch.nn`模块构建的。网络本身被写成一个继承自`nn.Module`的类，在这个类中，我们将使用`nn`来构建层。以下是 PyTorch [文档](https://pytorch.org/tutorials/)中的一个简单实现:

```
from torch import nn

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
```

尽管深入研究这些层是什么、它们是如何工作的以及如何实现它们已经超出了本文的范围，但是让我们快速探究一下上面的代码是做什么的。

*   `nn.Flaten`负责将多维数据转换为一维数据。

*   `nn.Sequential`容器在网络内部创建一系列层。

*   在容器内部，我们有几层。每种类型的层都以不同的方式转换数据，在神经网络中有多种方式来实现这些层。

*   前进函数是模型执行时调用的函数；但是，我们不应该直接调用它。

下面一行实例化了我们的模型:

```
model = NeuralNetwork()
print(model)
```

```
 NeuralNetwork(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (linear_relu_stack): Sequential(
        (0): Linear(in_features=784, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=10, bias=True)
      )
    )
```

## 训练神经网络

现在我们已经定义了我们的神经网络，我们可以把它投入使用。在开始训练之前，我们应该首先设置一个损失函数。损失函数衡量我们的模型离正确结果有多远，这也是我们在网络训练期间试图最小化的。交叉熵是用于分类任务的常见损失函数，也是我们将要使用的函数。我们应该初始化函数:

```
loss_function = nn.CrossEntropyLoss()
```

训练前的最后一步是设置一个优化算法。这种算法将负责在训练过程中调整模型，以便最小化由我们上面选择的损失函数测量的误差。这种任务的常见选择是随机梯度下降算法。然而，PyTorch 有其他几种可能性，你可以在这里熟悉[。下面是代码:](https://pytorch.org/docs/stable/optim.html)

```
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
```

`lr`参数是学习率，它代表了在训练的每次迭代中模型参数更新的速度。

最后，是训练和测试网络的时候了。对于这些任务中的每一个，我们将实现一个函数。训练函数包括一次遍历一批数据，使用优化器调整模型，以及计算预测和损失。这是 PyTorch 的标准实现:

```
def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        pred = model(X)
        loss = loss_fn(pred, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 1000 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
```

注意，对于每一次迭代，我们都获得数据来为模型提供数据，而且还要跟踪批次号，这样我们就可以每 100 次迭代打印一次损耗和当前批次。

然后是测试函数，它计算精度和损耗，这次使用测试集:

```
def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```

然后，我们设置历元数来训练我们的模型。一个历元由数据集上的一次迭代组成。例如，如果我们设置`epochs=5` ，这意味着我们将对整个数据集进行 5 次神经网络训练和测试。我们训练得越多，结果就越好。

这是 PyTorch 的实现和这样一个循环的输出:

```
epochs = 5
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(loaded_train, model, loss_function, optimizer)
    test(loaded_test, model, loss_function)
print("Done!")
```

```
 Epoch 1
    -------------------------------
    loss: 2.296232  [    0/60000]
    Test Error: 
     Accuracy: 47.3%, Avg loss: 2.254638 

    Epoch 2
    -------------------------------
    loss: 2.260034  [    0/60000]
    Test Error: 
     Accuracy: 63.2%, Avg loss: 2.183432 

    Epoch 3
    -------------------------------
    loss: 2.173747  [    0/60000]
    Test Error: 
     Accuracy: 66.9%, Avg loss: 2.062604 

    Epoch 4
    -------------------------------
    loss: 2.078938  [    0/60000]
    Test Error: 
     Accuracy: 72.4%, Avg loss: 1.859960 

    Epoch 5
    -------------------------------
    loss: 1.871736  [    0/60000]
    Test Error: 
     Accuracy: 75.8%, Avg loss: 1.562622 
```

```
 Done!
```

请注意，在每个时期，我们在训练循环中每 100 批打印一次损失函数，它一直变得越来越低。此外，在每个时期之后，我们可以看到，随着平均损耗的降低，精度越来越高。

如果我们设置更多的纪元——比如说 10 个、50 个甚至 100 个——我们可能会看到更好的结果，但输出会更长，更难可视化和理解。

随着我们的模型最终定型，保存它并在必要时加载它变得很容易:

```
torch.save(model, "model.pth")
model = torch.load("model.pth")
```

## 结论

在本文中，我们介绍了使用 PyTorch 进行深度学习的基础知识，包括:

*   张量及其使用方法

*   如何加载和准备数据

*   神经网络以及如何在 PyTorch 上定义它们

*   如何训练你的第一个图像分类模型

*   在 PyTorch 上保存和加载模型